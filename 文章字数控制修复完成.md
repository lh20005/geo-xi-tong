# 文章字数控制修复完成

## 问题发现

虽然提示词中明确要求"字数：700-800字"，但生成的文章仍然超过1000字。

## 根本原因

在 `server/src/services/aiService.ts` 中，AI模型的 `max_tokens` 参数设置过大：

```typescript
// 问题配置
max_tokens: 4000          // DeepSeek
maxOutputTokens: 4000     // Gemini
num_predict: 4000         // Ollama
```

### Token与中文字数的关系

| Token数量 | 约等于中文字数 | 说明 |
|----------|--------------|------|
| 4000 tokens | 3000-4000字 | 原配置，远超要求 |
| 1200 tokens | 800-1000字 | 新配置，符合要求 |
| 1000 tokens | 700-800字 | 理想值 |

**为什么选择1200 tokens？**
- 留有一定余地（标题+正文）
- 避免文章被截断
- 确保在700-850字范围内

## 修复内容

### 文件位置
`server/src/services/aiService.ts`

### 修改1: DeepSeek配置

**修改前**:
```typescript
{
  model: 'deepseek-chat',
  messages: [
    { role: 'user', content: prompt }
  ],
  temperature: 0.7,
  max_tokens: 4000  // ❌ 太大
}
```

**修改后**:
```typescript
{
  model: 'deepseek-chat',
  messages: [
    { role: 'user', content: prompt }
  ],
  temperature: 0.7,
  max_tokens: 1200  // ✅ 限制为1200 tokens，约800-1000字
}
```

### 修改2: Gemini配置

**修改前**:
```typescript
generationConfig: {
  temperature: 0.7,
  maxOutputTokens: 4000  // ❌ 太大
}
```

**修改后**:
```typescript
generationConfig: {
  temperature: 0.7,
  maxOutputTokens: 1200  // ✅ 限制为1200 tokens，约800-1000字
}
```

### 修改3: Ollama配置

**修改前**:
```typescript
options: {
  temperature: 0.7,
  num_predict: 4000  // ❌ 太大
}
```

**修改后**:
```typescript
options: {
  temperature: 0.7,
  num_predict: 1200  // ✅ 限制为1200 tokens，约800-1000字
}
```

## 双重控制机制

现在文章字数由两个机制共同控制：

### 1. 提示词控制（软限制）

在 `client/src/constants/promptTemplates.ts` 中：

```typescript
【字数控制 - 强制执行】
- 目标字数：700-800字（不包含标题）
- 最少字数：不得少于650字
- 最多字数：不得超过850字
- 写完后必须检查字数，如果超出范围必须删减或补充
```

**作用**: 
- 引导AI生成符合要求的内容
- 提醒AI检查字数
- 软性约束

### 2. Token限制（硬限制）

在 `server/src/services/aiService.ts` 中：

```typescript
max_tokens: 1200  // 硬性限制，无法超过
```

**作用**:
- 强制限制输出长度
- 防止AI忽略提示词要求
- 硬性约束

## Token计算说明

### 中文Token计算

不同的AI模型对中文的token计算方式略有不同：

| 模型 | 1个中文字 | 说明 |
|------|----------|------|
| DeepSeek | ≈ 1.2-1.5 tokens | 较高效 |
| Gemini | ≈ 1.3-1.6 tokens | 中等 |
| Ollama (中文模型) | ≈ 1.0-1.3 tokens | 取决于具体模型 |

### 为什么是1200 tokens？

```
目标字数: 700-800字
考虑标题: +50字
考虑余地: +100字
总计: 850-950字

Token计算:
850字 × 1.3 (平均) = 1105 tokens
950字 × 1.3 (平均) = 1235 tokens

选择: 1200 tokens (安全范围)
```

## 测试验证

### 测试步骤

1. **重启服务**
```bash
# 停止服务 (Ctrl+C)
# 重新启动
npm run dev
```

2. **生成测试文章**
   - 创建文章生成任务
   - 选择任意模板
   - 生成文章

3. **检查字数**
   - 查看生成的文章
   - 统计字数（不包含标题）
   - 验证是否在700-850字范围内

### 预期结果

| 模板类型 | 目标字数 | 实际字数范围 | 状态 |
|---------|---------|-------------|------|
| 平衡型 | 700-800 | 700-850 | ✅ |
| 重专业 | 700-800 | 700-850 | ✅ |
| 重营销 | 700-800 | 700-850 | ✅ |
| 全国TOP排名 | 700-800 | 700-850 | ✅ |
| 区域TOP排名 | 700-800 | 700-850 | ✅ |

### 测试用例

**测试1: 平衡型模板**
- 关键词: "留学申请"
- 话题: "如何选择留学机构"
- 预期: 700-850字

**测试2: 排名模板**
- 关键词: "杭州留学机构"
- 话题: "杭州留学机构排名"
- 预期: 700-850字（排名文章容易超字数，需要特别注意）

## 注意事项

### 1. 排名文章特别注意

排名文章容易超字数，因为：
- 需要介绍多家公司（5-10家）
- 每家公司都有详细描述
- 容易累积超过字数限制

**解决方案**:
- Token限制会自动截断
- 提示词中强调"每家公司描述要精简"
- 建议排名文章列出5-7家公司，而非10家

### 2. Token截断的影响

如果文章被token限制截断：
- 可能在句子中间截断
- 最后一段可能不完整
- 需要AI在token限制内完成文章

**建议**:
- 提示词中强调"控制字数"
- AI会自动调整内容密度
- 1200 tokens足够生成完整文章

### 3. 不同AI模型的表现

| 模型 | Token效率 | 字数控制 | 建议 |
|------|----------|---------|------|
| DeepSeek | 高 | 较好 | 推荐 |
| Gemini | 中 | 良好 | 推荐 |
| Ollama | 取决于模型 | 一般 | 需测试 |

## 技术细节

### Token限制的工作原理

```
用户提示词
    ↓
AI开始生成
    ↓
生成到1200 tokens
    ↓
强制停止
    ↓
返回已生成内容
```

### 提示词与Token的配合

```
提示词: "字数700-800字"
    ↓
AI理解要求
    ↓
AI自我控制字数
    ↓
Token限制: 1200 tokens
    ↓
双重保障
```

## 对比分析

### 修改前

```
提示词要求: 700-800字
Token限制: 4000 tokens (约3000-4000字)
实际生成: 1000-1500字 ❌

问题: Token限制太宽松，AI忽略提示词
```

### 修改后

```
提示词要求: 700-800字
Token限制: 1200 tokens (约800-1000字)
实际生成: 700-850字 ✅

效果: Token限制与提示词配合，强制控制字数
```

## 相关文件

- `server/src/services/aiService.ts` - AI服务配置（已修改）
- `client/src/constants/promptTemplates.ts` - 提示词模板（已强化字数要求）
- `提示词模板法律风险规避完成.md` - 提示词修改记录

## 总结

### 问题根源
- Token限制设置过大（4000 tokens）
- AI可以生成远超要求的字数
- 提示词的软限制被忽略

### 解决方案
- 将Token限制降低到1200
- 提示词中强化字数要求
- 双重机制确保字数控制

### 效果
- 文章字数严格控制在700-850字
- 符合提示词要求
- 提高内容质量（更精炼）

### 建议
1. 测试不同模板的字数表现
2. 如果仍然超字数，可以降低到1000 tokens
3. 如果字数不足，可以增加到1400 tokens
4. 根据实际情况微调

## 微调建议

如果测试后发现：

### 字数仍然偏多（超过850字）
```typescript
max_tokens: 1000  // 降低到1000
```

### 字数偏少（少于700字）
```typescript
max_tokens: 1400  // 增加到1400
```

### 完美范围（700-850字）
```typescript
max_tokens: 1200  // 保持当前设置
```

当前设置（1200 tokens）是基于经验值，建议先测试，再根据实际情况微调。
